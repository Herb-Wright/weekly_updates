---
title: Herbie's Weekly Update üóìÔ∏è
date: 2026 Feb 13
bibliography: 2026-02-13/refs.bib
link-citations: true
---

## 1. Last Time

Last time, we spent most of the meeting talking about the parallelization. This week, I have a small recap of that, then I also ran some experiments for the tip-cube task, where I have implemented the reweighting, but found reweighting to be not very important for the task, but we probably shouldn't draw broad conclusions. Finally, I also talk about some other stuff.

## 2. (Revisiting) Timing Results

This section simply summarizes our looking into timing results for Cholesky solves. The experiments here are generated by running various Cholesky decompositions and solves on random matrices with dimensions similar to those in the push-cube/pivot-cube tasks.

**Takeaway 1:** *Having JAX compile to CPU is significantly slower than GPU for all batch sizes.*

![Comparison of GPU vs CPU compiled Cholesky Decomp+Solves in JAX](image.png)

**Takeaway 2:** *The `cho_solve` part seems to saturate at around 32-64 samples.*

Unfortunately, it doesn't seem like there is a great way to get around this. Here are a couple plots that show the saturation:

![Cholesky solve (not including decomposition) saturates on GPU](image-1.png)

**Takeaway 3:** *Manually calling `triangular_solve` with an explicit batch dimension doesn't really help at all.*

![Comparisons of different ways to do the Cholesky solve](image-2.png)

## 3. Adding in Reweighting

In order to implement reweighting into the ADMM algorithm, we start with the objective:
$$ \min_{\pi} \mathbb E_{\xi \sim P(\xi)} [ c(x(\xi), u(\xi)) ] $$
We instead consider if we have some sort of observation that we are adjusting our belief in $\xi$ with:
$$ \min_{\pi} \mathbb E_{\xi \sim P(\xi|o_{1:T_\text{hist}})} [ c( x(\xi),  u(\xi)) ] $$
Using Bayes rule, we can convert this to:
$$ \min_{\pi} \mathbb E_{\xi \sim P(\xi)} \left[P(o_{1:T_\text{hist}}|\xi) c( x(\xi),  u(\xi)) \right] $$
After turning this into our MPC objective, we have:
$$ \min_{\substack{ x_{0:T}^{(1:N)},  u_{0:T-1}^{(1:N)}, \lambda_{0:T-1}^{(1:N)}, \\ K_{0:T-1},  v_{0:T-1}}} \; \sum_{i=1}^N P(o_{1:T_\text{hist}}|\xi) \left[ \left\| x^{(i)}_T\right\|_{Q_f}^2 + \sum_{t=0}^{T-1}  \left\|  x^{(i)}_t \right\|_Q^2 + \left\|  u^{(i)}_t \right\|^2_R \right] $$
$$ \text{ s.t. } \;  x_{t+1}^{(i)} = f_{\xi^{(i)}} ( x_t^{(i)},  u_t^{(i)}, \lambda_t^{(i)}) $$
$$ 0 \leq \lambda_t^{(i)} \perp \Phi_{\xi^{(i)}}( x_t^{(i)},  u_t^{(i)}, \lambda_t^{(i)}) \geq 0 $$
$$   u_t^{(i)} = K_t  x_t^{(i)} +  v_t $$ 
In practice, this really only affects the QP step, where $P(o_{1:T_\text{hist}} | \xi )$ balances the cost matrices with the augmented Lagrangian matching terms. A low likelihood corresponds to the matching terms having a larger effect. Also, as I mentioned previously, I am simply using the likelihood:
$$ P(o_{1:T_\text{hist}} | \xi ) \propto \exp \left(- \gamma \sum_t \| \hat x - x_o \|^2 \right) $$
Where $\hat x$ is gathered from rolling the LCS forward. Because of this, the LCS rolling forward can be combined with the initial stepping of the LCS for the ADMM optimization for added efficiency. I have implemented all of this and here is an example of what the reweighting probabilities look like at various time points:

![Results of reweighting](image-3.png)

After adding all this in, I was able to keep the time for the whole controller loop to be from 0.048-0.052 (s).

## 4. Tip Cube Experiments

I ran an ablation experiment on the tip-cube task. For the experiment, I slightly randomized the starting position of the ball, the rotation of the block, and the dimensions of the block. Then I tested taking away key elements of the method by running the experiment 100 times each. I call a run a success if the block's rotation is within 0.25 radians of the desired rotation after 2 seconds of simulation. Here are the results:

| Method         | Success Rate | Avg. error (rad) |
| -------------- | ------------ | ---------------- |
| No Steering    | 28%          | 0.4166           |
| No Reweighting | 70%          | 0.2839           |
| Full Method    | **71%**      | **0.2777**       |

As you can see, the steering does seem to be important, but the reweighting doesn't seem to be much of a factor and the difference could easily be described by statistical noise. I am not sure a 71% success rate is that good, so this might need to be revisited. I should note that I do not give the methods the true size of the block beforehand, and instead simply give them a distribution to be robust to. I tried to examine how $\gamma$ (hyperparameter for reweighting) affects the performance, and it really doesn't affect it that much:

![Effect of $\gamma$ on success. $\gamma=0$ is equivalent to no reweighting, and $\gamma = 10.0$ resulted in NaNs regularly.](image-4.png)

I mean, perhaps there is an argument that a correctly tuned $\gamma$ (maybe about $\gamma = 10^{-3}$) will result in statistically significant improvement if we have enough trials‚Äîit resulted in a success rate of 75%, which was the highest.

The next step here would be to compare with a sampling-based approach like MPPI. We can even compare against a robust version of it [@abraham2020model; @pezzato2025sampling].

I also reran the above experiment using $\gamma=0.001$ and doing 1000 trials instead, and got the following results (¬± is 95% CI):

| Method         | Success Rate | Avg. error (rad)   |
| -------------- | ------------ | ------------------ |
| No Steering    | 28.8%        | 0.4126 ¬±0.0109     |
| No Reweighting | 72.2%        | 0.2716 ¬±0.0097     |
| Full Method    | **72.8%**    | **0.2700** ¬±0.0096 |

Clearly, the result with respect to steering is significant, but the one with respect to reweighting is not.

My untested hypothesis is that $\gamma$ would need to be much larger to matter much, but that when $\gamma$ is big enough, it creates numerical issues in solving the QP, which just cancels out any benefit.

## 5. Towards a Tri-finger Reorientation Task in Sim

The next step is to do a simulated task of a trifinger reorienting an object. There exists a trifinger URDF/XML file for Mujoco that I can use:

![Mujoco trifinger setup](./trifinger.gif)

I just need to implement the correct contact detection so that only contacts between the object and fingertips are considered, and unnecessary contacts are not. I would also like to do the ADMM algorithm in joint torques instead of fingertip positions.

## 6. Other Minor Stuff

### 6.1. Robot Learning Class

This week, quite a bit of time got eaten up preparing for the robot learning class I am taking, where I have been assigned to be a debater on Monday. It is about simulation and its role in robotics.

### 6.2. Writing the Paper

I figure I am at the part of the project, where I could begin a draft of the paper. I am hoping to make significant progress on this next week.

## References




