<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="" >

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
            <title>Weekly Update</title>
  <style>
    /*
     * I add this to html files generated with pandoc.
     */

     html {
        font-size: 100%;
        overflow-y: scroll;
        -webkit-text-size-adjust: 100%;
        -ms-text-size-adjust: 100%;
    }

    body {
        color: #444;
        font-family: sans-serif;
        font-size: 12px;
        line-height: 1.7;
        padding: 1em;
        margin: auto;
        max-width: 800px;
        background: #fefefe;
        padding-bottom: 10rem;
        text-align: justify;
    }

    a {
        color: #0645ad;
        text-decoration: none;
    }

    a:visited {
        color: #0b0080;
    }

    a:hover {
        color: #06e;
    }

    a:active {
        color: #faa700;
    }

    a:focus {
        outline: thin dotted;
    }

    *::-moz-selection {
        background: rgba(255, 255, 0, 0.3);
        color: #000;
    }

    *::selection {
        background: rgba(255, 255, 0, 0.3);
        color: #000;
    }

    a::-moz-selection {
        background: rgba(255, 255, 0, 0.3);
        color: #0645ad;
    }

    a::selection {
        background: rgba(255, 255, 0, 0.3);
        color: #0645ad;
    }

    p {
        margin: 1em 0;
    }

    img {
        max-height: 350px;
        max-width: 100%;
    }

    h1, h2, h3, h4, h5, h6 {
        color: #111;
        line-height: 125%;
        margin-top: 2em;
        font-weight: normal;
    }

    h4, h5, h6 {
        font-weight: bold;
    }

    h1 {
        font-size: 2.5em;
    }

    h2 {
        font-size: 2em;
    }

    h3 {
        font-size: 1.5em;
    }

    h4 {
        font-size: 1.2em;
    }

    h5 {
        font-size: 1em;
    }

    h6 {
        font-size: 0.9em;
    }

    blockquote {
        color: #666666;
        margin: 0;
        padding-left: 3em;
        border-left: 0.5em #EEE solid;
    }

    hr {
        display: block;
        height: 2px;
        border: 0;
        border-top: 1px solid #aaa;
        border-bottom: none;
        margin: 1em 0;
        padding: 0;
    }

    pre, code, kbd, samp {
        color: #111;
        font-family: Consolas, monospace, monospace;
        _font-family: 'courier new', monospace;
        font-size: 0.98em;
        background-color: #f2f1f1;
        padding: 0.2em;
        border-radius: 0.25em;
    }

    pre {
        white-space: pre;
        white-space: pre-wrap;
        word-wrap: break-word;
    }

    b, strong {
        font-weight: bold;
    }

    dfn {
        font-style: italic;
    }

    ins {
        background: #ff9;
        color: #000;
        text-decoration: none;
    }

    mark {
        background: #ff0;
        color: #000;
        font-style: italic;
        font-weight: bold;
    }

    sub, sup {
        font-size: 75%;
        line-height: 0;
        position: relative;
        vertical-align: baseline;
    }

    sup {
        top: -0.5em;
    }

    sub {
        bottom: -0.25em;
    }

    ul, ol {
        margin: 1em 0;
        padding: 0 0 0 2em;
    }

    li {
        margin-bottom: 0.25em;
    }

    li:last-child p:last-child {
        margin-bottom: 0;
    }

    ul ul, ol ol {
        margin: 0;
    }

    dl {
        margin-bottom: 1em;
    }

    dt {
        font-weight: bold;
        margin-bottom: .8em;
    }

    dd {
        margin: 0 0 .8em 2em;
    }

    dd:last-child {
        margin-bottom: 0;
    }

    img {
        border: 0;
        -ms-interpolation-mode: bicubic;
        vertical-align: middle;
    }

    figure {
        display: block;
        text-align: center;
        margin: 1em 0;
    }

    figure img {
        border: none;
        margin: 0 auto;
    }

    figcaption {
        font-size: 0.8em;
        font-style: italic;
        margin: 0 0 .8em;
    }

    table {
        margin-bottom: 2em;
        border-bottom: 1px solid #ddd;
        border-right: 1px solid #ddd;
        border-spacing: 0;
        border-collapse: collapse;
    }

    table th {
        padding: .2em 1em;
        background-color: #eee;
        border-top: 1px solid #ddd;
        border-left: 1px solid #ddd;
    }

    table td {
        padding: .2em 1em;
        border-top: 1px solid #ddd;
        border-left: 1px solid #ddd;
        vertical-align: top;
    }

    .author {
        font-size: 1.2em;
        text-align: center;
    }

    span.display {
        overflow: auto;
        max-width: 100%;
        display: block;
    }

    #title-block-header {
        text-align: left;
        margin-bottom: 4em;
    }

    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  

    @media only screen and (min-width: 480px) {
        body {
            font-size: 14px;
        }
    }
    @media only screen and (min-width: 768px) {
        body {
            font-size: 16px;
        }
    }
    @media print {
        * {
            background: transparent !important;
            filter: none !important;
            -ms-filter: none !important;
        }

        body {
            font-size: 10pt;
            line-height: 1.5;
            max-width: 100%;
            padding-bottom: 0pt;
            color: black;
        }

        /* a, a:visited {
            text-decoration: underline;
        } */

        hr {
            height: 1px;
            border: 0;
            border-bottom: 1px solid black;
        }

        /* a[href]:after {
            content: " (" attr(href) ")";
        } */
    /* 
        a {
            color: var(--sky-800);
        } */

        abbr[title]:after {
            content: " (" attr(title) ")";
        }

        .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
            content: "";
        }

        pre, blockquote {
            border: 1px solid #999;
            padding-right: 1em;
            page-break-inside: avoid;
        }

        tr, img {
            page-break-inside: avoid;
        }

        img {
            max-width: 100% !important;
            max-height: 150pt;
        }

        p, h2, h3 {
            orphans: 3;
            widows: 3;
        }

        h2, h3 {
            page-break-after: avoid;
        }

        h1, h2, h3, h4, h5, h6 {
            margin-top: 1.25em;
            margin-bottom: 0.5em;
        }

        p, li, #refs {
            margin: 0.5em 0;
            font-size: 11pt;
        }

        li {
            margin-top: 0em;
            margin-bottom: 0.25em;
        }

        .printIgnore, #TOC  {
            display: none;
        }

        table {
            border: 1pt solid #444;
            /* border-right: 1pt solid #444; */
        }
        
        table th {
            /* border-top: 1pt solid #444; */
            border: 1pt solid #444;
        }
        
        table td {
            /* border-top: 1pt solid #444;
            border-left: 1pt solid #444; */
            border-top: none;
            /* border-bottom: none; */
            border-left: 1pt solid #444;
        }
    }


    pre > code.sourceCode { white-space: pre; position: relative; }
        pre > code.sourceCode > span { line-height: 1.25; }
        pre > code.sourceCode > span:empty { height: 1.2em; }
        .sourceCode { overflow: visible; }
        code.sourceCode > span { color: inherit; text-decoration: inherit; }
        div.sourceCode { margin: 1em 0; }
        pre.sourceCode { margin: 0; }
        @media screen {
        div.sourceCode { overflow: auto; }
        }
        @media print {
        pre > code.sourceCode { white-space: pre-wrap; }
        pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
        }
        pre.numberSource code
          { counter-reset: source-line 0; }
        pre.numberSource code > span
          { position: relative; left: -4em; counter-increment: source-line; }
        pre.numberSource code > span > a:first-child::before
          { content: counter(source-line);
            position: relative; left: -1em; text-align: right; vertical-align: baseline;
            border: none; display: inline-block;
            -webkit-touch-callout: none; -webkit-user-select: none;
            -khtml-user-select: none; -moz-user-select: none;
            -ms-user-select: none; user-select: none;
            padding: 0 4px; width: 4em;
            background-color: #ffffff;
            color: #a0a0a0;
          }
        pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
        div.sourceCode
          { color: #1f1c1b; background-color: #ffffff; }
        @media screen {
        pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
        }
        code span { color: #1f1c1b; } /* Normal */
        code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
        code span.an { color: #ca60ca; } /* Annotation */
        code span.at { color: #0057ae; } /* Attribute */
        code span.bn { color: #b08000; } /* BaseN */
        code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
        code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
        code span.ch { color: #924c9d; } /* Char */
        code span.cn { color: #aa5500; } /* Constant */
        code span.co { color: #898887; } /* Comment */
        code span.cv { color: #0095ff; } /* CommentVar */
        code span.do { color: #607880; } /* Documentation */
        code span.dt { color: #0057ae; } /* DataType */
        code span.dv { color: #b08000; } /* DecVal */
        code span.er { color: #bf0303; text-decoration: underline; } /* Error */
        code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
        code span.fl { color: #b08000; } /* Float */
        code span.fu { color: #644a9b; } /* Function */
        code span.im { color: #ff5500; } /* Import */
        code span.in { color: #b08000; } /* Information */
        code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
        code span.op { color: #1f1c1b; } /* Operator */
        code span.ot { color: #006e28; } /* Other */
        code span.pp { color: #006e28; } /* Preprocessor */
        code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
        code span.sc { color: #3daee9; } /* SpecialChar */
        code span.ss { color: #ff5500; } /* SpecialString */
        code span.st { color: #bf0303; } /* String */
        code span.va { color: #0057ae; } /* Variable */
        code span.vs { color: #bf0303; } /* VerbatimString */
        code span.wa { color: #bf0303; } /* Warning */  </style>
    <link rel="stylesheet" href="_template/styles.css" />
      <script defer=""
    src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
    var mathElements = document.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
      katex.render(texText.data, mathElements[i], {
        displayMode: mathElements[i].classList.contains('display'),
        throwOnError: false,
        macros: macros,
        fleqn: false
      });
    }}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Mono:wght@100..900&family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
</head>

<body>
      <header id="title-block-header">
        <h1 class="title">Weekly Update</h1>
                <p class="date">2025 Oct 17</p>
          </header>
      <nav id="TOC" role="doc-toc">
        <ul>
        <li><a href="#last-time" id="toc-last-time">1 Last Time</a></li>
        <li><a href="#towards-my-first-project-idea"
        id="toc-towards-my-first-project-idea">2 Towards my First
        Project Idea</a></li>
        <li><a href="#brainstorming-a-meam-5170-project"
        id="toc-brainstorming-a-meam-5170-project">3 Brainstorming a
        MEAM 5170 Project</a></li>
        <li><a href="#openai-api-keys" id="toc-openai-api-keys">4 OpenAI
        API Keys</a></li>
        <li><a href="#other-news" id="toc-other-news">5 Other
        News</a></li>
        <li><a href="#references"
        id="toc-references">References</a></li>
        </ul>
  </nav>
    <h2 id="last-time">1 Last Time</h2>
    <p>Last time, we discussed a few different ideas; namely the VLM
    idea we have been talking about and some robust MPC stuff. This
    time, because my computer came, I didn’t have any time to do any
    deep dives, as much of my time was spent setting up my new computer.
    However, now that I do have my computer, I want to start working on
    my original idea I pitched over the summer.</p>
    <h2 id="towards-my-first-project-idea">2 Towards my First Project
    Idea</h2>
    <h3 id="the-pitch">2.1 The Pitch</h3>
    <p>Last time I had a high-level overview of the pitch:</p>
    <blockquote>
    <ul>
    <li><strong>(Potential) Title:</strong> <em>Diverse and Physically
    Stable Bayesian World Models for Manipulation</em></li>
    <li><strong>Motivation:</strong> The ability to convert partial
    observations of a scene into reasonable estimates for the scene’s
    dynamics is incredibly useful in robotics. In the case of robotic
    manipulation, this usually means reconstructing the geometry of each
    object in the scene along with some physics parameters. The
    reconstructions ought to be physically stable and capture the
    diversity/uncertainty from occlusion. Such diverse, physically
    stable reconstructions from a single RGBD image can be used
    downstream by controllers for robotic manipulation.</li>
    <li><strong>Method:</strong> We can do Bayesian reconstruction using
    a BundleSDF-like <span class="citation"
    data-cites="wen2023bundlesdf"><a href="#ref-wen2023bundlesdf"
    role="doc-biblioref">[1]</a></span> ensemble. Use Amodal3R <span
    class="citation" data-cites="wu2025amodal3r"><a
    href="#ref-wu2025amodal3r" role="doc-biblioref">[2]</a></span> with
    some modulation as a prior, simplified probabilisitic ContactNets
    <span class="citation" data-cites="pfrommer2021contactnets"><a
    href="#ref-pfrommer2021contactnets"
    role="doc-biblioref">[3]</a></span> loss for physical stability.
    Then use likelihood from observation similar to the negative
    sampling in <span class="citation" data-cites="wright2024v"><a
    href="#ref-wright2024v" role="doc-biblioref">[4]</a></span>.</li>
    <li><strong>Experiments:</strong> We can perform simple pushing
    tasks to verify that our model is <em>accurate</em> via multi-object
    pushing, and <em>diverse</em> via pushing with some occlusion that
    creates multiple different possible outcomes from the same
    push.</li>
    </ul>
    </blockquote>
    <p>The key idea is to combine physical stability with diverse,
    probabilistic object reconstructions. There were two motivating
    examples I have for it:</p>
    <figure>
    <img src="image.png"
    alt="Motivating examples where diverse reconstruction affects physics and physical constraints" />
    <figcaption aria-hidden="true">Motivating examples where diverse
    reconstruction affects physics and physical constraints</figcaption>
    </figure>
    <p>Then, I also had a figure for the proposed method:</p>
    <figure>
    <img src="image-1.png" alt="overview of proposed method" />
    <figcaption aria-hidden="true">overview of proposed
    method</figcaption>
    </figure>
    <p>The mathematical formulation of things can be found in depth in
    my <a href="../2025-06-11_project_pitch">original pitch</a>.</p>
    <p>Obviously, we didn’t get around to discussing this last meeting,
    but there are a few changes since the original pitch; namely the
    idea to use Amodal3R + bounding box randomization to get diversity,
    which maybe has a similar insight to <span class="citation"
    data-cites="saund2021diverse"><a href="#ref-saund2021diverse"
    role="doc-biblioref">[5]</a></span>, but in 2D instead of 3D.</p>
    <p><strong>Question:</strong> <em>How does this pitch sound? What
    would strengthen it?</em></p>
    <h3 id="initial-steps">2.2 Initial Steps</h3>
    <p>The first step, in my estimation, is to get Amodal3R <span
    class="citation" data-cites="wu2025amodal3r"><a
    href="#ref-wu2025amodal3r" role="doc-biblioref">[2]</a></span>
    working on my new computer. There is <a
    href="https://github.com/Sm0kyWu/Amodal3R">a github repo</a> for it,
    which I have cloned, but I did not have time to actually get around
    to resolving all the dependency things to try it out. I had a sort
    of roadmap set out last time, and here were the first few of the
    items:</p>
    <blockquote>
    <ol type="1">
    <li>Getting Amodal3R+registration running (Minghan probably already
    has this, but I will want it working for me)
    <ol type="1">
    <li>Create a few example images</li>
    <li>Get Amodal3R running on new GPU</li>
    <li>Get registration working</li>
    <li>Get Amodal3R working with randomized bounding boxes</li>
    </ol></li>
    <li>Training a Bayesian ensemble implicit neural model with Amodal3R
    and depth supervision
    <ol type="1">
    <li>Negative sampling</li>
    <li>Bayesian ensemble …</li>
    </ol></li>
    </ol>
    </blockquote>
    <p><strong>Question:</strong> <em>How do we feel about just going
    for it on this project?</em></p>
    <p>There are probably going to be problems that pop up along the
    way, but I would like to preemptively aim for maybe like IROS next
    year deadline</p>
    <h2 id="brainstorming-a-meam-5170-project">3 Brainstorming a MEAM
    5170 Project</h2>
    <h2 id="openai-api-keys">4 OpenAI API Keys</h2>
    <p>This section just has some info from when I briefly looked into
    OpenAI API keys. Here are some of the popular OpenAI models and
    their pricing:</p>
    <table>
    <thead>
    <tr class="header">
    <th>Model</th>
    <th style="text-align: right;">Input $/1M Token</th>
    <th style="text-align: right;">Output $/1M Tokens</th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td>GPT-5</td>
    <td style="text-align: right;">1.25</td>
    <td style="text-align: right;">10.00</td>
    </tr>
    <tr class="even">
    <td>GPT-5 pro</td>
    <td style="text-align: right;">15.00</td>
    <td style="text-align: right;">120.00</td>
    </tr>
    <tr class="odd">
    <td>GPT-5 nano</td>
    <td style="text-align: right;">0.05</td>
    <td style="text-align: right;">0.40</td>
    </tr>
    <tr class="even">
    <td>GPT-4o</td>
    <td style="text-align: right;">2.50</td>
    <td style="text-align: right;">10.00</td>
    </tr>
    </tbody>
    </table>
    <p>It should be noted that the token prices are for <em>text</em>
    tokens, and images would likely have different pricing. There seems
    to be <a
    href="https://platform.openai.com/docs/quickstart/build-your-application">decent
    docs</a> on connecting to the API via python. I made an API key and
    tried to call the OpenAI API and got the following response:</p>
    <div class="sourceCode" id="cb1"><pre
    class="sourceCode json"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;error&quot;</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;message&quot;</span><span class="fu">:</span> <span class="st">&quot;You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.&quot;</span><span class="fu">,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;type&quot;</span><span class="fu">:</span> <span class="st">&quot;insufficient_quota&quot;</span><span class="fu">,</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;param&quot;</span><span class="fu">:</span> <span class="kw">null</span><span class="fu">,</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;code&quot;</span><span class="fu">:</span> <span class="st">&quot;insufficient_quota&quot;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">}</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code></pre></div>
    <p>But you can easily add payment methods and set usage limits in
    their dashboard:</p>
    <figure>
    <img src="image-2.png" alt="OpenAI dashboard for billing info" />
    <figcaption aria-hidden="true">OpenAI dashboard for billing
    info</figcaption>
    </figure>
    <h2 id="other-news">5 Other News</h2>
    <ul>
    <li></li>
    </ul>
    <h2 class="unnumbered" id="references">References</h2>
    <div id="refs" class="references csl-bib-body" role="list">
    <div id="ref-wen2023bundlesdf" class="csl-entry" role="listitem">
    <div class="csl-left-margin">[1] </div><div
    class="csl-right-inline">B. Wen, J. Tremblay, V. Blukis, S. Tyree,
    T. Müller, A. Evans, D. Fox, J. Kautz, and S. Birchfield,
    <span>“Bundlesdf: Neural 6-dof tracking and 3d reconstruction of
    unknown objects,”</span> in <em>Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition</em>, 2023,
    pp. 606–617.</div>
    </div>
    <div id="ref-wu2025amodal3r" class="csl-entry" role="listitem">
    <div class="csl-left-margin">[2] </div><div
    class="csl-right-inline">T. Wu, C. Zheng, F. Guan, A. Vedaldi, and
    T.-J. Cham, <span>“Amodal3r: Amodal 3d reconstruction from occluded
    2d images,”</span> <em>arXiv preprint arXiv:2503.13439</em>,
    2025.</div>
    </div>
    <div id="ref-pfrommer2021contactnets" class="csl-entry"
    role="listitem">
    <div class="csl-left-margin">[3] </div><div
    class="csl-right-inline">S. Pfrommer, M. Halm, and M. Posa,
    <span>“Contactnets: Learning discontinuous contact dynamics with
    smooth, implicit representations,”</span> in <em>Conference on robot
    learning</em>, 2021, pp. 2279–2291.</div>
    </div>
    <div id="ref-wright2024v" class="csl-entry" role="listitem">
    <div class="csl-left-margin">[4] </div><div
    class="csl-right-inline">H. Wright, W. Zhi, M. Johnson-Roberson, and
    T. Hermans, <span>“V-PRISM: Probabilistic mapping of unknown
    tabletop scenes,”</span> in <em>2024 IEEE/RSJ international
    conference on intelligent robots and systems (IROS)</em>, 2024, pp.
    1078–1085.</div>
    </div>
    <div id="ref-saund2021diverse" class="csl-entry" role="listitem">
    <div class="csl-left-margin">[5] </div><div
    class="csl-right-inline">B. Saund and D. Berenson, <span>“Diverse
    plausible shape completions from ambiguous depth images,”</span> in
    <em>Conference on robot learning</em>, 2021, pp. 1802–1813.</div>
    </div>
    </div>
  </body>

</html>